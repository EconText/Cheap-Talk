{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the script used to get some interesting data about tweets from specific consumer-facing companies and tweets from specific firm-facing companies. The goal was to see if there is any difference in the way they tweet which may include the hashtags they use, how long their tweets are, the age of their accounts, etc. Note that this was originally run on the server, where all tweet data is accessible.\n",
    "\n",
    "The outputs of this script are `consumer_facing_companies.csv` and `firm_facing_companies.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tweepy\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from twitter_api_xanda import TWITTER_API_BEARER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEN_YEARS_DATA_PATH = \"data/tweets/ten_years/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweet Info\n",
    "\n",
    "Fields we want for specified user\n",
    "- age of account\n",
    "- avg hashtags per tweet\n",
    "- number of unique hashtags\n",
    "- most popular hashtags (top 5)\n",
    "- average tweet length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = tweepy.Client(bearer_token=TWITTER_API_BEARER)\n",
    "\n",
    "USER_FIELDS = ['created_at', 'username']\n",
    "USER_FIELDS = ['created_at', 'description', 'entities', 'id', 'location', 'name', 'pinned_tweet_id', 'profile_image_url', 'protected', 'public_metrics', 'url', 'username', 'verified', 'withheld']\n",
    "usernames = [\"google\",\"netflix\"]\n",
    "# This isn't working for some reason\n",
    "user_objs = client.get_users(usernames=usernames, user_fields=USER_FIELDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average length per tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data_file_path = \"data/tweets/ten_years/netflix_tweets.csv\"\n",
    "netflix_df = pd.read_csv(tweet_data_file_path)\n",
    "netflix_df[\"tweet_length\"] = netflix_df.apply(lambda row: len(row[\"text\"]), axis=1)\n",
    "sum(netflix_df[\"tweet_length\"])/len(netflix_df[\"tweet_length\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average hashtags per tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_hashtags(df):\n",
    "    hashtags_list = []\n",
    "    \n",
    "    hashtags_series = df[df[\"hashtags\"].notnull()][\"hashtags\"]\n",
    "\n",
    "    for tags in hashtags_series:\n",
    "        cleaned_tags_list = tags.replace(\"{\", \"\").replace(\"}\", \"\").replace(\"'\", \"\").split(\", \")\n",
    "        hashtags_list.extend(cleaned_tags_list)\n",
    "\n",
    "    return hashtags_list\n",
    "\n",
    "len(get_all_hashtags(netflix_df))/len(netflix_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of unique hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(get_all_hashtags(netflix_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 5 most used hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags_list = get_all_hashtags(netflix_df)\n",
    "netflix_counter = Counter(hashtags_list)\n",
    "netflix_counter.most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_tweet_length(df: pd.DataFrame):\n",
    "    df[\"tweet_length\"] = df.apply(lambda row: len(row[\"text\"]), axis=1)\n",
    "    average_length = sum(df[\"tweet_length\"])/len(df[\"tweet_length\"])\n",
    "    \n",
    "    return average_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_hashtags(df):\n",
    "    hashtags_list = []\n",
    "    \n",
    "    hashtags_series = df[df[\"hashtags\"].notnull()][\"hashtags\"]\n",
    "\n",
    "    for tags in hashtags_series:\n",
    "        cleaned_tags_list = tags.replace(\"{\", \"\").replace(\"}\", \"\").replace(\"'\", \"\").split(\", \")\n",
    "        hashtags_list.extend(cleaned_tags_list)\n",
    "\n",
    "    return hashtags_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting all the info for our companies of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS = [\"name\", \"username\", \"avg_tweet_length\", \"num_unique_hashtags\", \"avg_hashtags_per_tweet\", \"most_common_hashtags\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONSUMER_FACING_COMPANIES = [\n",
    "    (\"Bath & Body Works, Inc.\", \"bathbodyworks\"),\n",
    "    (\"Chipotle Mexican Grill\", \"chipotletweets\"),\n",
    "    (\"Delta Air Lines\", \"delta\"),\n",
    "    (\"Disney\", \"waltdisneyco\"),\n",
    "    (\"Expedia Group\", \"ExpediaGroup\"),\n",
    "    (\"Ford Motor Company\", \"ford\"),\n",
    "    (\"Hilton Worldwide\", \"hiltonnewsroom\"),\n",
    "    (\"Kellogg's\", \"kelloggcompany\"),\n",
    "    (\"Netflix\", \"netflix\"),\n",
    "    (\"PepsiCo\", \"PepsiCo\")\n",
    "]\n",
    "\n",
    "FIRM_FACING_COMPANIES = [\n",
    "    (\"Broadcom Inc.\", \"Broadcom\"),\n",
    "    (\"Caterpillar Inc.\", \"caterpillarinc\"),\n",
    "    (\"Cisco\", \"Cisco\"),\n",
    "    (\"Dow Inc.\", \"DowNewsroom\"),\n",
    "    (\"Intel\", \"intel\"),\n",
    "    (\"Lockheed Martin\", \"lockheedmartin\"),\n",
    "    (\"Marathon Petroleum\", \"MarathonPetroCo\"),\n",
    "    (\"Moody's Corporation\", \"MoodysInvSvc\"),\n",
    "    # (\"Old Dominion\", \"odfl_inc\"),\n",
    "    (\"Salesforce\", \"salesforce\")\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tweet_stats_df(companies):\n",
    "    rows = []\n",
    "    for name, handle in companies:\n",
    "        company_df = pd.read_csv(f\"{TEN_YEARS_DATA_PATH}{handle}_tweets.csv\", lineterminator='\\n')\n",
    "        \n",
    "        len_per_tweet = get_avg_tweet_length(company_df)\n",
    "        hashtags_list = get_all_hashtags(company_df)\n",
    "        num_unique_hashtags = len(set(hashtags_list))\n",
    "        avg_hashtags_per_tweet = len(hashtags_list)/len(company_df)\n",
    "        \n",
    "        most_common_hashtags = Counter(hashtags_list).most_common(5)\n",
    "        most_common_hashtags_words = set([tag for tag, _ in most_common_hashtags])\n",
    "        \n",
    "        rows.append([name, handle, len_per_tweet, num_unique_hashtags, avg_hashtags_per_tweet, most_common_hashtags_words])\n",
    "        \n",
    "    output_df = pd.DataFrame(rows, columns=COLUMNS)\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_facing_companies_df = create_tweet_stats_df(CONSUMER_FACING_COMPANIES)\n",
    "consumer_facing_companies_df.to_csv(\"data/tweets/select_companies/consumer_facing_companies.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firm_facing_companies_df = create_tweet_stats_df(FIRM_FACING_COMPANIES)\n",
    "firm_facing_companies_df.to_csv(\"data/tweets/select_companies/firm_facing_companies.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "\n",
    "Here we create some word clouds for the hashtags. Note this will probably be biased toward companies that tweet more and use their own hashtags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handle = \"kelloggcompany\"\n",
    "company_df = pd.read_csv(f\"{TEN_YEARS_DATA_PATH}{handle}_tweets.csv\")\n",
    "hashtags_list = get_all_hashtags(company_df)\n",
    "hashtags_count_dict = Counter([tag.lower() for tag in hashtags_list])\n",
    "\n",
    "cloud = WordCloud(width = 800, height = 800,\n",
    "                background_color ='white',\n",
    "                stopwords = set(STOPWORDS),\n",
    "                min_font_size = 10).generate_from_frequencies(hashtags_count_dict)\n",
    " \n",
    "# plot the WordCloud image                      \n",
    "plt.figure(figsize = (8, 8), facecolor = None)\n",
    "plt.imshow(cloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0)\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handle = \"hiltonnewsroom\"\n",
    "company_df = pd.read_csv(f\"{TEN_YEARS_DATA_PATH}{handle}_tweets.csv\")\n",
    "hashtags_list = get_all_hashtags(company_df)\n",
    "hashtags_count_dict = Counter([tag.lower() for tag in hashtags_list])\n",
    "\n",
    "cloud = WordCloud(width = 800, height = 800,\n",
    "                background_color ='white',\n",
    "                stopwords = set(STOPWORDS),\n",
    "                min_font_size = 10).generate_from_frequencies(hashtags_count_dict)\n",
    " \n",
    "# plot the WordCloud image                      \n",
    "plt.figure(figsize = (8, 8), facecolor = None)\n",
    "plt.imshow(cloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0)\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handle = \"Cisco\"\n",
    "company_df = pd.read_csv(f\"{TEN_YEARS_DATA_PATH}{handle}_tweets.csv\")\n",
    "hashtags_list = get_all_hashtags(company_df)\n",
    "hashtags_count_dict = Counter([tag.lower() for tag in hashtags_list])\n",
    "\n",
    "cloud = WordCloud(width = 800, height = 800,\n",
    "                background_color ='white',\n",
    "                stopwords = set(STOPWORDS),\n",
    "                min_font_size = 10).generate_from_frequencies(hashtags_count_dict)\n",
    " \n",
    "# plot the WordCloud image                      \n",
    "plt.figure(figsize = (8, 8), facecolor = None)\n",
    "plt.imshow(cloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0)\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handle = \"MarathonPetroCo\"\n",
    "\n",
    "company_df = pd.read_csv(f\"{TEN_YEARS_DATA_PATH}{handle}_tweets.csv\")\n",
    "hashtags_list = get_all_hashtags(company_df)\n",
    "hashtags_count_dict = Counter([tag.lower() for tag in hashtags_list])\n",
    "\n",
    "cloud = WordCloud(width = 800, height = 800,\n",
    "                background_color ='white',\n",
    "                stopwords = set(STOPWORDS),\n",
    "                min_font_size = 10).generate_from_frequencies(hashtags_count_dict)\n",
    " \n",
    "# plot the WordCloud image                      \n",
    "plt.figure(figsize = (8, 8), facecolor = None)\n",
    "plt.imshow(cloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0)\n",
    " \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
